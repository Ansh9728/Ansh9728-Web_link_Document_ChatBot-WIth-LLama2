{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOb22MwpQF/lFwIQzpykQS8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ansh9728/Ansh9728-Web_link_Document_ChatBot-WIth-LLama2/blob/main/Project_Ansh9728_Web_link_Document_ChatBot_WIth_LLama2_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#all the required dependency\n",
        "!pip install bs4 -q\n",
        "!pip install langchain -q\n",
        "!pip install nltk -q\n",
        "!pip install faiss-cpu -q  # for CPU-based computation\n",
        "!pip install unstructured -q\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers -q\n",
        "!pip install pinecone-client -q\n",
        "!pip install tiktoken -q\n",
        "!pip install jq -q\n",
        "!pip install \"unstructured[pdf]\" -q\n",
        "!pip install \"unstructured[all-docs]\" -q\n",
        "!pip install pypdf -q\n",
        "!pip install bitsandbytes\n",
        "!pip install accelerate -q\n",
        "!pip install python-dotenv -q\n",
        "!pip install -q streamlit\n",
        "!npm install localtunnel -q"
      ],
      "metadata": {
        "id": "zHu5BlrvZ9v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sFNR9574adcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streamlit file\n"
      ],
      "metadata": {
        "id": "MgG7BDGEBLyY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRmSM8j56BKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada67b36-89a4-46b4-bc2c-d690c81e6781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n",
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import requests\n",
        "import re,os\n",
        "import sys,os,torch,string\n",
        "from bs4 import BeautifulSoup\n",
        "import nest_asyncio\n",
        "import nltk\n",
        "import string\n",
        "import time\n",
        "import tempfile\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader, UnstructuredExcelLoader\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Pinecone\n",
        "import pinecone\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from huggingface_hub import login\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from huggingface_hub import login\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "st.title(\"Web_Link & Document ChatBot\")\n",
        "\n",
        "input_options = st.radio(\n",
        "    \"What Input Do You Want to Provide?\", (\"Link\", \"Document\", \"Both\")\n",
        ")\n",
        "\n",
        "\n",
        "# Function Start to fetch the Web Data Element form the Web\n",
        "def get_links(website_link):\n",
        "    org_link = website_link\n",
        "    try:\n",
        "        response = requests.get(website_link)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            html_data = response.content\n",
        "            soup = BeautifulSoup(html_data, \"html.parser\")\n",
        "            for script in soup([\"script\", \"style\"]):\n",
        "                script.extract()\n",
        "            list_links = []\n",
        "            for link in soup.find_all(\"a\", href=True):\n",
        "                list_links.append(link[\"href\"])\n",
        "\n",
        "            list_links.append(org_link)\n",
        "            return list_links\n",
        "        else:\n",
        "            st.error(f\"Error code {response.status_code}\")\n",
        "            return []  # Return an empty list when there's an error\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        st.error(f\"RequestException: {e}\")\n",
        "        return []  # Return an empty list for any request exception\n",
        "    except Exception as e:\n",
        "        st.error(f\"An error occurred: {e}\")\n",
        "        return []  # Return an empty list for any other exception\n",
        "\n",
        "\n",
        "def filter_link(link_ls):\n",
        "    length = len(link_ls)\n",
        "    if length > 1:\n",
        "        url_pattern = re.compile(\n",
        "            r\"https?://(?!.*(?:\\.pdf|facebook\\.com|twitter\\.com|github\\.com))\\S+\"\n",
        "        )\n",
        "        unique_links = list(set())\n",
        "        # Iterate through the list of links and extract valid URLs\n",
        "        for link in link_ls:\n",
        "            match = re.search(url_pattern, link)\n",
        "            if match:\n",
        "                unique_links.append(match.group())\n",
        "        return unique_links\n",
        "    else:\n",
        "        return link_ls\n",
        "\n",
        "\n",
        "def scrap_web(url):\n",
        "    nest_asyncio.apply()\n",
        "    loader = WebBaseLoader(url)\n",
        "\n",
        "    loader.requests_per_second = 1\n",
        "    data = loader.aload()\n",
        "    return data\n",
        "\n",
        "\n",
        "def clean_page_content(page_content):\n",
        "    cleaned_content = \" \".join(page_content.split())\n",
        "    cleaned_content = \"\".join(\n",
        "        [char for char in cleaned_content if char not in string.punctuation]\n",
        "    )\n",
        "    words = cleaned_content.split()  # Tokenize the text\n",
        "    # stop_words = set(stopwords.words(\"english\"))# Remove stopwords\n",
        "    stop_words = nltk.corpus.stopwords.words(\"english\")\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "    # Join the cleaned words back into a string\n",
        "    cleaned_content = \" \".join(words)\n",
        "    return cleaned_content\n",
        "\n",
        "\n",
        "def clean_scrap_data(scrap_data):\n",
        "    cleaned_data = []\n",
        "\n",
        "    for document in scrap_data:\n",
        "        cleaned_document = (\n",
        "            document.copy()\n",
        "        )  # Create a copy to avoid modifying the original data\n",
        "        cleaned_document.page_content = clean_page_content(\n",
        "            cleaned_document.page_content\n",
        "        )\n",
        "        cleaned_data.append(cleaned_document)\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "\n",
        "# this function give me data from the website scrap\n",
        "def web_data(url):\n",
        "    with st.status(\"Feteching the WebSite Data\", expanded=True):\n",
        "        st.write(\"Finding Links\")\n",
        "        all_link_web = get_links(url)\n",
        "        time.sleep(2)\n",
        "        st.write(\"Cleaning the Links\")\n",
        "        # getting only the relevent link\n",
        "        filtered_links = filter_link(all_link_web)\n",
        "        len_of_filter_link = len(filtered_links)\n",
        "\n",
        "        time.sleep(2)\n",
        "        st.write(\"No of Links to Scrap are :\", len_of_filter_link)\n",
        "        time.sleep(1)\n",
        "        st.write(\"Scrapping the Website Data\")\n",
        "        scrap_data = scrap_web(filtered_links)  # scrapping the data\n",
        "\n",
        "        if scrap_data:\n",
        "            st.write(\"Scrapping Completed\")\n",
        "        else:\n",
        "            st.write(\"No Data Scrap\")\n",
        "\n",
        "    cleaned_data = clean_scrap_data(scrap_data)  # clean the scrap Data\n",
        "\n",
        "    return cleaned_data\n",
        "\n",
        "\n",
        "# Function End Web Data Element\n",
        "\n",
        "\n",
        "url = \"\"\n",
        "Folder_path = \"\"\n",
        "\n",
        "data = list()\n",
        "\n",
        "\n",
        "# first button initialize\n",
        "\n",
        "if \"submit_button\" not in st.session_state:\n",
        "    st.session_state.submit_button = False\n",
        "\n",
        "\n",
        "def callback():\n",
        "    st.session_state.submit_button = True\n",
        "    st.session_state.Get_Result = True\n",
        "\n",
        "\n",
        "if \"Link\" in input_options:\n",
        "    url = st.text_input(\"Enter the Website Url\")\n",
        "    if st.button(\"Submit\", on_click=callback) or st.session_state.submit_button:\n",
        "        single_data_file = web_data(url)\n",
        "        # st.write(single_data_file)\n",
        "        data.extend(single_data_file)\n",
        "\n",
        "if \"Document\" in input_options:\n",
        "    uploaded_files = st.file_uploader(\n",
        "        \"Upload a File\",\n",
        "        type=[\"txt\", \"pdf\", \"xlsx\", \"xls\"],\n",
        "        accept_multiple_files=True,\n",
        "    )\n",
        "\n",
        "    if st.button(\"submit\", on_click=callback) or st.session_state.submit_button:\n",
        "        if uploaded_files is not None:\n",
        "            # data = []\n",
        "            for file in uploaded_files:\n",
        "                temp_file_path = None\n",
        "\n",
        "                try:\n",
        "                    # Save the uploaded file to a temporary file\n",
        "                    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "                        temp_file.write(file.read())\n",
        "                        temp_file_path = temp_file.name\n",
        "\n",
        "                    # Process different file types using appropriate loaders\n",
        "                    if file.type == \"text/plain\":\n",
        "                        loader = TextLoader(temp_file_path, autodetect_encoding=True)\n",
        "                    elif file.type == \"application/pdf\":\n",
        "                        loader = PyPDFLoader(temp_file_path)\n",
        "                    elif file.type == \"text/csv\":\n",
        "                        # loader = UnstructuredCSVLoader(temp_file_path,mode=\"elements\")\n",
        "                        loader = CSVLoader(temp_file_path, encoding=True)\n",
        "                    elif (\n",
        "                        file.type\n",
        "                        == \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
        "                    ):\n",
        "                        loader = UnstructuredExcelLoader(temp_file_path)\n",
        "                    else:\n",
        "                        st.error(f\"Unsupported file format: {file.type}\")\n",
        "                        continue\n",
        "\n",
        "                    single_data_file = loader.load()\n",
        "                    # st.write(f\"Data from {file.name}: {data}\")\n",
        "                    single_data_file = clean_scrap_data(\n",
        "                        single_data_file\n",
        "                    )  # cleaning the scrap Data\n",
        "                    data.extend(single_data_file)\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error processing {file.name}: {str(e)}\")\n",
        "\n",
        "                finally:\n",
        "                    # Clean up temporary file after processing\n",
        "                    if temp_file_path is not None and os.path.exists(temp_file_path):\n",
        "                        os.remove(temp_file_path)\n",
        "            # st.write(data)\n",
        "    # st.session_state.submit_button=False\n",
        "\n",
        "\n",
        "if \"Both\" in input_options:\n",
        "    url = st.text_input(\"Enter the Website Url\")\n",
        "    uploaded_files = st.file_uploader(\n",
        "        \"Upload a File\",\n",
        "        type=[\"txt\", \"pdf\", \"xlsx\", \"xls\"],\n",
        "        accept_multiple_files=True,\n",
        "    )\n",
        "    if st.button(\"submit\", on_click=callback) or st.session_state.submit_button:\n",
        "        if url:\n",
        "            single_data_file = web_data(url)\n",
        "            # st.write(single_data_file)\n",
        "            data.extend(single_data_file)\n",
        "        else:\n",
        "            print(\"You Dont provide any url\")\n",
        "\n",
        "        if uploaded_files is not None:\n",
        "            # data = []\n",
        "            for file in uploaded_files:\n",
        "                temp_file_path = None\n",
        "\n",
        "                try:\n",
        "                    # Save the uploaded file to a temporary file\n",
        "                    with tempfile.NamedTemporaryFile(delete=False) as temp_file:\n",
        "                        temp_file.write(file.read())\n",
        "                        temp_file_path = temp_file.name\n",
        "\n",
        "                    # Process different file types using appropriate loaders\n",
        "                    if file.type == \"text/plain\":\n",
        "                        loader = TextLoader(temp_file_path, autodetect_encoding=True)\n",
        "                    elif file.type == \"application/pdf\":\n",
        "                        loader = PyPDFLoader(temp_file_path)\n",
        "                    elif file.type == \"text/csv\":\n",
        "                        # loader = UnstructuredCSVLoader(temp_file_path,mode=\"elements\")\n",
        "                        loader = CSVLoader(temp_file_path, encoding=True)\n",
        "                    elif (\n",
        "                        file.type\n",
        "                        == \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
        "                    ):\n",
        "                        loader = UnstructuredExcelLoader(temp_file_path)\n",
        "                    else:\n",
        "                        st.error(f\"Unsupported file format: {file.type}\")\n",
        "                        continue\n",
        "\n",
        "                    single_data_file = loader.load()\n",
        "                    # st.write(f\"Data from {file.name}: {data}\")\n",
        "                    single_data_file = clean_scrap_data(\n",
        "                        single_data_file\n",
        "                    )  # cleaning the scrap Data\n",
        "                    data.extend(single_data_file)\n",
        "\n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error processing {file.name}: {str(e)}\")\n",
        "\n",
        "                finally:\n",
        "                    # Clean up temporary file after processing\n",
        "                    if temp_file_path is not None and os.path.exists(temp_file_path):\n",
        "                        os.remove(temp_file_path)\n",
        "\n",
        "        # st.session_state.submit_button=False\n",
        "\n",
        "# #Here we get the Data to further proceess\n",
        "#st.write(len(data))\n",
        "#st.write(data)\n",
        "\n",
        "# End OF LOADING DATA FILES\n",
        "\n",
        "\n",
        "# START TASK CREATING CHUNKS\n",
        "def split_docs(documents):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        # Set a really small chunk size, just to show.\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len,\n",
        "        is_separator_regex=False,\n",
        "    )\n",
        "    docs = text_splitter.split_documents(documents)\n",
        "    return docs\n",
        "\n",
        "\n",
        "split_docs_data = split_docs(data)\n",
        "st.write(\"length of doc\", len(split_docs_data))\n",
        "\n",
        "# END TASK CREATING CHUNKS\n",
        "\n",
        "# START THE VECTOR DATABASE INTEGRATION\n",
        "global index_name\n",
        "def db_embeddings(\n",
        "    docs,\n",
        "    api_key=\"54c47713-e05a-4360-b511-e6b47b899c43\",\n",
        "    environment=\"gcp-starter\",\n",
        "    index_name=\"langchain-doc1-embed\",\n",
        "):\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    pinecone.init(api_key=api_key, environment=environment)\n",
        "\n",
        "    index_name = index_name\n",
        "\n",
        "    if index_name not in pinecone.list_indexes():\n",
        "        pinecone.create_index(\n",
        "            name=index_name,\n",
        "            metric=\"cosine\",\n",
        "            dimension=len(embeddings.embed_query(\"hello\")),\n",
        "        )\n",
        "\n",
        "    Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
        "    # index = Pinecone.from_documents(docs, embeddings, index_name=index_name)\n",
        "\n",
        "\n",
        "# getting the Pinecone Api keys and enviroment to create the enviroment\n",
        "\n",
        "try:\n",
        "    if split_docs_data:\n",
        "        with st.form(key=\"pinecone creditional\"):\n",
        "            st.info(\n",
        "                \"\"\"\n",
        "            Please Provide the following Details:\n",
        "            Pinecone API key, Index Name, and Environment of Pinecone\n",
        "\n",
        "            If you Don't want to Provide Just leave it blank By default Values Used.\n",
        "\n",
        "            \"\"\"\n",
        "            )\n",
        "            api_key = st.text_input(\"Enter the API key\")\n",
        "            index_name = st.text_input(\"Enter the Index Name\")\n",
        "            environment = st.text_input(\"Enter the Environment\")\n",
        "            parameters = {\n",
        "                \"api_key\": api_key,\n",
        "                \"index_name\": index_name,\n",
        "                \"environment\": environment,\n",
        "            }\n",
        "\n",
        "            submitted = st.form_submit_button(\"Submit\")\n",
        "\n",
        "            if submitted:\n",
        "                st.write(\"Embeddings Started\")\n",
        "                db_embeddings(\n",
        "                    docs=split_docs_data,\n",
        "                    **{key: value for key, value in parameters.items() if value},\n",
        "                )\n",
        "                st.write(\"Vector Embeddings Done in Pinecone\")\n",
        "                st.session_state.submit_button = False\n",
        "\n",
        "except Exception as e:\n",
        "    st.write(f\"Error Present in that are {str(e)}\")\n",
        "\n",
        "# END OF VECTOR DATA BASE INTEGRATION\n",
        "\n",
        "# MODEL RELATED TASK AND FUNCTION\n",
        "\n",
        "# GETTING THE SIMILAR DOCUMENT\n",
        "def get_similiar_docs(question, k=3, score=True):\n",
        "    index_name = \"langchain-doc1-embed\"\n",
        "    embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    index = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)\n",
        "    index = Pinecone.from_existing_index(index_name=index_name, embedding=embeddings)\n",
        "    if score:\n",
        "        similar_docs = index.similarity_search_with_score(question, k=k)\n",
        "    else:\n",
        "        similar_docs = index.similarity_search(question, k=k)\n",
        "\n",
        "    return similar_docs\n",
        "# END OF SIMILAR DOCUMENT FUNCTION\n",
        "\n",
        "#hugging face login\n",
        "login(\"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\")\n",
        "\n",
        "#FUNCTION TO LOAD THE ML MODEL\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "model_name=\"meta-llama/Llama-2-7b-chat-hf\"\n",
        "#quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model(model_name):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name,device_map='auto',load_in_8bit=True,torch_dtype=torch.float32)\n",
        "\n",
        "  pipe=pipeline(\n",
        "      'text-generation',\n",
        "      model=model,\n",
        "      tokenizer=tokenizer,\n",
        "      max_new_tokens=250,\n",
        "      model_kwargs={'temperature':0.2},\n",
        "\n",
        "  )\n",
        "\n",
        "  llm = HuggingFacePipeline(pipeline=pipe,verbose=True,callback_manager=callback_manager)\n",
        "\n",
        "  return llm\n",
        "\n",
        "llm = load_model(model_name)\n",
        "\n",
        "chat_history = {'conversation': []}\n",
        "#st.write(chat_history)\n",
        "def get_result_streamlit_chat(question):\n",
        "    similar_doc = get_similiar_docs(question)\n",
        "    #print(similar_doc)\n",
        "    similar_doc = ''.join([(i[0].page_content) for i in similar_doc])\n",
        "\n",
        "    if \"messages\" not in st.session_state:\n",
        "        st.session_state.messages = []\n",
        "\n",
        "    for message in st.session_state.messages:\n",
        "        #st.write(message)\n",
        "        with st.chat_message(message[\"role\"]):\n",
        "            st.markdown(message[\"content\"])\n",
        "            #chat_history = {'conversation': [message]}\n",
        "\n",
        "    chat_history = {'conversation': [st.session_state.messages]}\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    <s>[INST] <<SYS>>\n",
        "            Use the following pieces of context and chat history to answer the question at the end.\n",
        "\n",
        "            Only Provide an answer without additional information.\n",
        "\n",
        "            ### If you don't know the answer or the question is out of context, just say that you don't know.\n",
        "\n",
        "            Use three sentences maximum and keep the answer as concise as possible\n",
        "\n",
        "    </<<SYS>>\n",
        "\n",
        "    context: {similar_doc}\n",
        "\n",
        "    chat history: {chat_history['conversation']}\n",
        "\n",
        "    question: {question}\n",
        "\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "    #st.write(prompt)\n",
        "    res=llm(prompt)\n",
        "    #chat_history['conversation'].append({'question': question, 'response': res})\n",
        "\n",
        "    return res\n",
        "\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "#for message in st.session_state.messages:\n",
        "#    with st.chat_message(message[\"role\"]):\n",
        "#        st.markdown(message[\"content\"])\n",
        "\n",
        "if question := st.chat_input(\"Enter Your Question?\"):\n",
        "    # Display user message in chat message container\n",
        "    #st.chat_message(\"user\").markdown(question)\n",
        "    # Add user message to chat history\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": question})\n",
        "\n",
        "    response=get_result_streamlit_chat(question)\n",
        "    # Display assistant response in chat message container\n",
        "    with st.chat_message(\"assistant\"):\n",
        "       st.markdown(response)\n",
        "    # Add assistant response to chat history\n",
        "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "\n",
        "processing_done = False\n",
        "\n",
        "if not processing_done:\n",
        "    st.info('Click the Exit Button to Delete Vector DATABASE')\n",
        "    if st.button(\"Exit\"):\n",
        "      index_name = \"langchain-doc1-embed\"\n",
        "      pinecone.delete_index(index_name)\n",
        "      st.success(f\"Pinecone index {index_name} has been deleted.\")\n",
        "      processing_done = True  # Set the flag to indicate processing is done\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lXHfPFefag2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the stramlit app With local Tunnel\n",
        "Running the Notebook Please Change the RunTime"
      ],
      "metadata": {
        "id": "gbDACkdBalTf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFYeLMhW6lA_"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & #Storing the logs details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcyWwhZ76oU7",
        "outputId": "462b07ca-9fbe-4b17-9a2b-c0034ee88cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.240.140.2\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.961s\n",
            "your url is: https://slimy-things-try.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com  #this is the ip address generate to chek streamlit app\n",
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KMRnpAN3bDAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dS-QB_m_bDgu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}